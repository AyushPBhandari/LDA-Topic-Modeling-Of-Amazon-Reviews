{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from prep import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "We will implement various unsupervised methods on Amazon Fine Food Reviews [1]. The dataset consists of reviews of fine foods from Amazon. The overview of the dataset is as at the end of this document.\n",
    "\n",
    "In this project, we want to cluster the dataset into two clusters: positive review and negative review. To do that we will do dimensionality reduction using Principal Component Analysis, t-sne and Latent Dirichlet Allocation, and compare the results to know which method works better for this case and discuss why it works or not. For the clustering, we will try various clustering methods we have learnt in class. For example: k-means, GMM, or DBScan. Again, we will discuss the result to know which clustering methods work best for this kind of data. For the data pre-processing on the review dataset, we will follow the standard procedure on text, which are stopwords removal and stemming and then convert it into vectors of tf-idf or bag of words.\n",
    "\n",
    "Furthermore on this analysis, we will not only utilize the reviews from the user, but also we will use a transaction data to see what combinations of products a user usually buy using FP-Growth algorithm.\n",
    "\n",
    "[1] http://snap.stanford.edu/data/web-FineFoods.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 1 - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning:\n",
    "1. Remove duplicated reviews:  \n",
    "We can see the  the dataset has duplication reviews for one review.\n",
    "Remove empty reviews. Also for one review, two records have different Helpfulness Denominator and Helpfulness Numerator. In that case, we assume the record is cumulated and only kept the record with largest number of Helpfulness Denominator.  \n",
    "2. Remove the empty reviews:\n",
    "The purpose of the project is to do semantic analysis for customer reviews. When reviews are not applicable, that review is not helpful. We remove the empty review.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have cleaned our numerical and categorical datasets and made it as pickle, so that we can just load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Id   ProductId          UserId                      ProfileName  \\\n",
       "0  1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1  2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2  3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3  4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4  5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "  HelpfulnessNumerator HelpfulnessDenominator Score        Time  \\\n",
       "0                    1                      1     5  1303862400   \n",
       "1                    0                      0     1  1346976000   \n",
       "2                    1                      1     4  1219017600   \n",
       "3                    3                      3     2  1307923200   \n",
       "4                    0                      0     5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_reviews_df = pd.read_pickle(\"./dataframes/All_reviews.pkl\")\n",
    "all_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def polarity(x):\n",
    "    if x < 3:\n",
    "        # Means negative\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        # Means positive\n",
    "        return \"Positive\"\n",
    "    \n",
    "all_reviews_df[\"ScoreAsCategory\"] = all_reviews_df[\"Score\"].map(polarity)\n",
    "\n",
    "ax = all_reviews_df[\"ScoreAsCategory\"].value_counts().plot(kind=\"bar\")\n",
    "ax.set_xlabel(\"Review\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will do text and summary pre-processing, both will be stored to two text files text_prep.txt and summary_prep.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n"
     ]
    }
   ],
   "source": [
    "rtitle = re.compile(r'^review\\/text:\\s*(.+)')\n",
    "stemmer = SnowballStemmer('english')\n",
    "texts = []\n",
    "stopwords = []\n",
    "parsed_titles = []\n",
    "\n",
    "with open('stopwords_english.txt') as fsw:\n",
    "    for word in fsw.readlines():\n",
    "        word = word.strip('\\n')\n",
    "        stopwords.append(word)\n",
    "fsw.close()\n",
    "\n",
    "with open('finefoods.txt', encoding='latin-1') as fin:\n",
    "    for line in fin.readlines():\n",
    "        line = line.strip('\\n')\n",
    "        mtext = rtitle.match(line)\n",
    "        ## if it is a title string\n",
    "        if mtext:\n",
    "            text = mtext.group(1).lower()\n",
    "            texts.append(text[:-1])\n",
    "fin.close()\n",
    "\n",
    "fout = open('text_prep.txt', 'w+')\n",
    "for i, words in enumerate(texts):\n",
    "    words = words.split(\" \")\n",
    "    words = [re.match('[a-zA-Z0-9]+', stemmer.stem(word)).group() for word in words if re.match('[a-zA-Z0-9]+', stemmer.stem(word)) is not None]\n",
    "    words = ['NUM' if re.match('[0-9]+', word) is not None else word for word in words]\n",
    "    words = list(filter(None, [\"\" if word in stopwords else word for word in words]))\n",
    "    words = ' '.join(words)\n",
    "    fout.write(words + '\\n')\n",
    "    parsed_titles.append(words)\n",
    "    if i % 100000 == 0 :\n",
    "        print(i)\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n"
     ]
    }
   ],
   "source": [
    "rtitle = re.compile(r'^review\\/summary:\\s*(.+)')\n",
    "stemmer = SnowballStemmer('english')\n",
    "texts = []\n",
    "stopwords = []\n",
    "parsed_titles = []\n",
    "\n",
    "with open('stopwords_english.txt') as fsw:\n",
    "    for word in fsw.readlines():\n",
    "        word = word.strip('\\n')\n",
    "        stopwords.append(word)\n",
    "fsw.close()\n",
    "\n",
    "with open('finefoods.txt', encoding='latin-1') as fin:\n",
    "    for line in fin.readlines():\n",
    "        line = line.strip('\\n')\n",
    "        mtext = rtitle.match(line)\n",
    "        ## if it is a title string\n",
    "        if mtext:\n",
    "            text = mtext.group(1).lower()\n",
    "            texts.append(text[:-1])\n",
    "fin.close()\n",
    "\n",
    "fout = open('summary_prep.txt', 'w+')\n",
    "for i, words in enumerate(texts):\n",
    "    words = words.split(\" \")\n",
    "    words = [re.match('[a-zA-Z0-9]+', stemmer.stem(word)).group() for word in words if re.match('[a-zA-Z0-9]+', stemmer.stem(word)) is not None]\n",
    "    words = ['NUM' if re.match('[0-9]+', word) is not None else word for word in words]\n",
    "    words = list(filter(None, [\"\" if word in stopwords else word for word in words]))\n",
    "    words = ' '.join(words)\n",
    "    fout.write(words + '\\n')\n",
    "    parsed_titles.append(words)\n",
    "    if i % 100000 == 0 :\n",
    "        print(i)\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load dataset and convert bag of words represenetations.\n",
    "\"\"\"\n",
    "with open(\"./text_prep.txt\", \"r\") as t:\n",
    "    df = t.read()\n",
    "    df = df.split(\"\\n\")\n",
    "\n",
    "vec = CountVectorizer(min_df=800, max_features=1023)\n",
    "df = vec.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 496),\n",
       " ('tast', 887),\n",
       " ('good', 380),\n",
       " ('flavor', 345),\n",
       " ('one', 603),\n",
       " ('love', 514),\n",
       " ('product', 689),\n",
       " ('use', 953),\n",
       " ('great', 388),\n",
       " ('veri', 963)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will see the top 10 words appears the most\n",
    "\n",
    "count_df = df.toarray().sum(axis=0)\n",
    "vocab = {v: k for k, v in vec.vocabulary_.items()}\n",
    "[(vocab[i], vec.vocabulary_[vocab[i]]) for i in count_df.argsort()[-10:][::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to run LDA over our dataset for 10 topics to see the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topwords_pca(cv, pca, num_topwords):\n",
    "    \"\"\"\n",
    "    print the top words based on the eigenvectors \n",
    "    \"\"\"\n",
    "    Topwords = []\n",
    "    vocab = {v: k for k, v in cv.vocabulary_.items()}\n",
    "    eigenvectors = pca.components_\n",
    "    for k,ev in enumerate(eigenvectors):\n",
    "        ev_indices = np.argsort(- np.abs(ev))[:num_topwords]\n",
    "        ev_topwords =', '.join([vocab[ind] for ind in ev_indices])\n",
    "        Topwords.append(ev_topwords)\n",
    "        print(\"Component : %d, topwords : %s\" % (k, ev_topwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "Components: 10\n",
      "======================================================\n",
      "Component : 0, topwords : bar, love, eat, free, gluten, snack, protein, tast, great, organ\n",
      "Component : 1, topwords : use, salt, mix, butter, make, tast, peanut, like, great, popcorn\n",
      "Component : 2, topwords : love, find, amazon, store, great, use, buy, price, local, get\n",
      "Component : 3, topwords : like, tast, flavor, chocol, good, chip, cooki, eat, snack, bag\n",
      "Component : 4, topwords : order, product, amazon, price, box, buy, veri, good, ship, purchas\n",
      "Component : 5, topwords : flavor, hot, make, great, good, like, veri, love, add, tast\n",
      "Component : 6, topwords : tea, coffe, flavor, cup, like, coffee, tast, good, drink, one\n",
      "Component : 7, topwords : dog, treat, get, love, one, like, time, day, use, give\n",
      "Component : 8, topwords : tast, like, use, water, product, drink, sugar, flavor, oil, tri\n",
      "Component : 9, topwords : food, cat, dog, eat, like, feed, love, chicken, one, dri\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=10, n_jobs=1)\n",
    "lda.fit(df)\n",
    "# reduced_X = lda.transform(df)\n",
    "print(\"======================================================\")\n",
    "print(\"Components: {0}\".format(10))\n",
    "print(\"======================================================\")\n",
    "topwords_pca(vec, lda, num_topwords=10)\n",
    "print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the category for each topic, for example the first topic talks about snack, most likelike protein bar like and while the second topic is lilely about salt and butter. We couldn't tell much from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load dataset and convert bag of words represenetations.\n",
    "\"\"\"\n",
    "with open(\"./text_prep.txt\", \"r\") as t:\n",
    "    df = t.read()\n",
    "    df = df.split(\"\\n\")\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=800, max_features=1023)\n",
    "df_tfidf = tfidf.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try to make the text as TF-IDF to see if it makes a difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "Components: 10\n",
      "======================================================\n",
      "Component : 0, topwords : use, make, cook, add, great, tast, like, good, rice, mix\n",
      "Component : 1, topwords : amazon, price, store, find, great, buy, love, bar, local, product\n",
      "Component : 2, topwords : tea, flavor, green, hot, tast, like, drink, good, sauc, love\n",
      "Component : 3, topwords : coffe, coffee, cup, flavor, like, strong, tast, roast, good, blend\n",
      "Component : 4, topwords : dog, food, cat, treat, love, eat, like, one, get, chew\n",
      "Component : 5, topwords : chip, salt, popcorn, flavor, like, tast, bag, chips, good, great\n",
      "Component : 6, topwords : chocol, tast, like, flavor, candi, sugar, bar, sweet, good, syrup\n",
      "Component : 7, topwords : order, product, veri, box, arriv, receiv, amazon, time, packag, great\n",
      "Component : 8, topwords : cooki, gluten, love, like, eat, free, tast, great, good, bread\n",
      "Component : 9, topwords : drink, water, tast, like, use, product, tea, bottl, help, good\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=10, n_jobs=1)\n",
    "lda.fit(df_tfidf)\n",
    "# reduced_X = lda.transform(df)\n",
    "print(\"======================================================\")\n",
    "print(\"Components: {0}\".format(10))\n",
    "print(\"======================================================\")\n",
    "topwords_pca(vec, lda, num_topwords=10)\n",
    "print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
